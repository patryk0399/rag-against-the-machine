from __future__ import annotations

import os
from typing import Annotated, Literal

from typing_extensions import TypedDict

from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage, AIMessage, ToolMessage
from langchain_core.tools import tool

from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode
from langgraph.checkpoint.memory import MemorySaver

from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings

from src.config import AppConfig, load_config
from src.llm_backend import get_local_llm
from agents.tools import search as search_function
from rag.query import retrieve #as rag_answer  # underlying RAG function

# --- Debug helpers -----------------------------------------------------------
DEBUG_FLOW = os.getenv("DEBUG_FLOW", "1") == "1"


def _dbg(*parts):
    if DEBUG_FLOW:
        print("[dbg]", *parts)


def _msg_brief(msg: BaseMessage) -> str:
    t = type(msg).__name__
    content = getattr(msg, "content", "")
    if isinstance(content, str):
        snippet = content.replace("\n", " ")
        if len(snippet) > 140:
            snippet = snippet[:1000] + "â€¦"
    else:
        snippet = str(content)
    tc = getattr(msg, "tool_calls", None)
    if tc:
        return f"{t}(tool_calls={len(tc)}) :: {snippet}"
    return f"{t} :: {snippet}"


def _dump_messages(messages: list[BaseMessage], label: str = "messages") -> None:
    _dbg(f"{label}: count={len(messages)}")
    if not DEBUG_FLOW:
        return
    for i, m in enumerate(messages):
        _dbg(f"  [{i}]", _msg_brief(m))
        tc = getattr(m, "tool_calls", None) or []
        for j, call in enumerate(tc):
            _dbg(
                f"     tool_call[{j}] name={call.get('name')} args={call.get('args')} id={call.get('id')}"
            )


# ---------------------------------------------------------------------------

# Example embeddings + FAISS store loader (adjust to your project paths)
#embeddings = HuggingFaceEmbeddings(model_name=os.getenv("EMBEDDINGS_MODEL", "sentence-transformers/all-MiniLM-L6-v2"))
#index_root = os.getenv("FAISS_INDEX_ROOT", "./faiss_index")


# @tool
# def get_vector_store(query: str, k: int = 4) -> str:
#     """Search the local FAISS vector store and return the top-k documents (as a string)."""
#     vector_store = FAISS.load_local(
#         folder_path=str(index_root),
#         embeddings=embeddings,
#         allow_dangerous_deserialization=True,
#     )
#     docs = vector_store.similarity_search(query, k=k)
#     return "\n\n".join([f"[doc{i}] {d.page_content}" for i, d in enumerate(docs)])



tools = [search_function, retrieve]
tool_node = ToolNode(tools)


class AgentState(TypedDict):
    messages: Annotated[list[BaseMessage], add_messages]


def _choose_forced_tool(last_user_text: str) -> str | None:
    """Router (concept), replace later.
    must force `tool_choice`.
    """
    print("[GOING INTO FORCED TOOL SELECTION]")
    t = (last_user_text or "").lower()

    if any(x in t for x in ["search"]):
        return "search"

    if any(x in t for x in ["document", "docs", "context"]):
        return "retrieve"

    return None


def chatbot_node(state: AgentState) -> AgentState:
    """answer directly OR force a tool call (ChatLlamaCpp)."""
    print("[GOING INTO CHATBOT]")
    #print("[AGENT STATE ATTR] ", getattr())
    _dbg("ENTER chatbot_node")
    _dump_messages(state["messages"], label="state.messages (incoming)")

    system = SystemMessage(
        content=(
            "You are a helpful assistant. "
            "If a tool is forced, call it using the model's native tool-calling mechanism. "
            "Otherwise answer normally or use the provided context."
        )
    )
    messages = [system, *state["messages"]]

    # Decide whether to force a tool call.
    # IMPORTANT: only force tool_choice on the first hop (when the latest message is Human).
    last_msg = state["messages"][-1] if state.get("messages") else None
    forced_tool: str | None = None
    if isinstance(last_msg, HumanMessage):
        forced_tool = _choose_forced_tool(last_msg.content)

    cfg = load_config()
    llm = get_local_llm(cfg=cfg)

    last_msg = state["messages"][-1]
    print("====================================== ", type(last_msg).__name__)
    if isinstance(last_msg, AIMessage):
        print("[GET FINISH REASON BEFORE LLM.INVOKE] ", last_msg.response_metadata.get("finish_reason"))
    if isinstance(last_msg, ToolMessage):
        print("<<<<<<<<<<LASTMSG>>>>>>>>>>>>> : ", last_msg)
        print("====== LAST IS TOOL")
    else:
        print("====== LAST IS NOT (!!!) TOOL")

    if forced_tool and state:
        # if lastIsTool:
        #     messages = state["messages"][-1]
        _dbg("chatbot_node: forcing tool_choice =", forced_tool)
        # Force one tool because of ChatLlamaCpp limitations
        llm_with_tools = llm.bind_tools(
            tools,
            tool_choice={"type": "function", "function": {"name": forced_tool}},
        )
        response = llm_with_tools.invoke(messages)
    elif isinstance(last_msg, ToolMessage):
        _dbg("chatbot_node: last message is ToolMessage. Inovking LLM with messages + RAG context", forced_tool)
        context_msg = f"User query: {messages}\n Context from retrieval: {last_msg}"
        response = llm.invoke(context_msg)
    else:
        _dbg("chatbot_node: no tool forced; answering directly")
        response = llm.invoke(messages)

    
    print("================", type(response).__name__)
    if(type(response).__name__ == "AIMessage"):
        print("[GET FINISH REASON AFTER LLM.INVOKE] ", response.response_metadata["finish_reason"])# state["messages"][0][0])#[0]["finish_reason"])

    _dbg("EXIT chatbot_node")
    _dbg("llm response type=", type(response).__name__)
    _dbg("llm response tool_calls=", getattr(response, "tool_calls", None))
    _dbg("llm response content_snippet=", (response.content or "").replace("\n", " ")[:1000])
    return {"messages": [response]}


def should_continue(state: AgentState) -> Literal["tools", "end"]:
    last = state["messages"][-1]
    tc = getattr(last, "tool_calls", None)
    decision = "tools" if (isinstance(last, AIMessage) and tc) else "end"
    _dbg("[should_continue:]", decision, "| last=", _msg_brief(last))
    return decision


def build_graph(memory):
    cfg = load_config()

    _dbg("build_graph:", "model_path=", getattr(cfg, "llm_model_path", None))
    _dbg("build_graph:", "tools=", [getattr(t, "name", type(t).__name__) for t in tools])

    graph_builder = StateGraph(AgentState)

   # from functools import partial
    graph_builder.add_node("chatbot", chatbot_node)
    graph_builder.add_node("tools", tool_node)

    graph_builder.add_edge(START, "chatbot")
    graph_builder.add_conditional_edges("chatbot", should_continue, {"tools": "tools", "end": END})
    graph_builder.add_edge("tools", "chatbot")

    return graph_builder.compile(checkpointer=memory)


def chat_with_memory(message: str, graph, thread_id: str):
    print("[GOING INTO CHAT WITH MEMORY]")
    config = {"configurable": {"thread_id": thread_id}, "recursion_limit": 10}
    initial_state = {"messages": [HumanMessage(content=message)]}

    _dbg("chat_with_memory:", f"thread_id={thread_id}")
    _dump_messages(initial_state["messages"], label="initial_state.messages")

    result = graph.invoke(initial_state, config)
    _dbg("chat_with_memory: graph.invoke returned")
    _dump_messages(result["messages"], label="result.messages")

    last = result["messages"][-1]
    print("AI:", last.content)


def main() -> None:
    memory = MemorySaver()
    graph = build_graph(memory)

    while True:
        user_input = input("User: ")
        if user_input.strip() == ":q":
            break
        chat_with_memory(user_input, graph, "thread-1")


if __name__ == "__main__":
    main()